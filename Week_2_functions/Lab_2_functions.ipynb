{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Lab_2_functions.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries that we need to import - numpy and json (for loading the description file)\n",
    "import numpy as np\n",
    "import json as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Looking for answers in all the right places\n",
    "\n",
    "Hopefully at this point you've figured out how the assignments are structured. This is a light-weight problem to fill in any gaps about where to look for help if you get stuck and/or figure out what is required for each problem\n",
    "\n",
    "Reminder that links to all of the slides for the week, supplementary videos (including a Short and Sweet (S&S) video per assignment) in the module in Canvas. The S&S videos are only a few minutes long and it is definitely worth skimming through them to get an overview of the assignment.\n",
    "\n",
    "In this problem we're going to work through each of the autograder tests and \"find\" the solution(s) for each test in turn. Run the tests right now; they'll all fail. \n",
    "\n",
    "TODO 1: Using the autograder to tell you the answer you should get. \n",
    "- Run the test. What error does the first test give? Why?\n",
    "- Create a variable based on the test. Set that variable to be a number initially.\n",
    "- Run the test again, and see if you can figure out what number to set the variable to\n",
    "- How \"close\" to the right answer do you have to be?\n",
    "- Also try setting the variable to be a string, list, numpy array - what error messages do you get? See if you can figure out what the error message means and why you're getting it. \n",
    "\n",
    "You should absolutely be using the autograder \"test\" to know what form/value the problem expects as an answer.\n",
    "\n",
    "TODO 2: Refering to the slides\n",
    "- Go to the top of the file. Click on the slide link. Do a search for the name in the test. \n",
    "- Find the slide describing second test. Follow the instructions there to create the list. \n",
    "- Go to gradescope and look at the rubric for Code checks. This is an example of an automatically graded answer but a manually graded check for \"following the instructions\". If you just do variable=answer you will get points deducted\n",
    "- Always check the slides and the rubric for instructions and common mistakes\n",
    "\n",
    "TODO 3: Using the variables window\n",
    "- Click on the Variables window\n",
    "- Find the variable that has the word \"find\" in it's name\n",
    "- What sentance does that variable have in it? Type the answer into the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer to the first test here\n",
    "mystery_variable= 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144]\n"
     ]
    }
   ],
   "source": [
    "# Your answer to the second test here\n",
    "my_fibonacci=[1,1]\n",
    "for item in range (10):\n",
    "    my_fibonacci.append(my_fibonacci[-1]+my_fibonacci[-2])\n",
    "print(my_fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer to the third test here\n",
    "\n",
    "# Type your answer to the question in this comment (Manually graded)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>mystery_question</pre></strong> passed! üåà</p>"
      ],
      "text/plain": [
       "mystery_question results: All test cases passed!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"mystery_question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Statistical analysis of data using numpy\n",
    "\n",
    "Lab slides: https://docs.google.com/presentation/d/1ykwwcQ0onMvAjUxfJmKl9tbo-rJPdB5pRwDEmpDsd-g/edit?usp=sharing\n",
    "\n",
    "For this lab the goal is to write functions to pull out one data channel and print out statistics for failed versus successful picks. This will involve using your function from the lecture activity to get just the data you want, then another function to do the statistics (essentially the **calc_stats** function from the lecture activity). \n",
    "\n",
    "Written properly, you only need one function to do stats for the entire data channel, just the succesful ones, or just the unsuccessful ones. For any data channel. In the homework you'll use these functions to do this for all of the data and write it back out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Reading in data\n",
    "\n",
    "TODO Copy over code to do the following:\n",
    "- read in the numerical data and pull out the numerical data and the successful/unsuccessful data\n",
    "- create a boolean index variable for the successful picks\n",
    "\n",
    "Note: For this lab I'm going to \"hard-wire\" all of the numbers (number of time steps, number of data dimensions, etc) to make testing easier. When you move this code over to the homework you'll replace all of the hard-wired numbers with the variables you're calculating in homework 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from lab 1\n",
    "file_name = \"Data/proxy_pick_data.csv\"\n",
    "try:\n",
    "    pick_data= np.loadtxt(file_name, dtype=\"float\", delimiter=\",\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file was not found; check that the data directory is in the current one and the file is in that directory\")\n",
    "\n",
    "pick_channenl_data = pick_data[:,:-1] \n",
    "pick_successful = pick_data[:,-1] \n",
    "b_successful = np.count_nonzero(pick_data[:, -1] == 1) \n",
    "\n",
    "# Hard-wiring these values for the testing code\n",
    "n_timesteps = 40\n",
    "n_picks = 660\n",
    "n_total_dims = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong style='color: red;'><pre style='display: inline;'>get_data</pre> results:</strong></p><p><strong><pre style='display: inline;'>get_data - 1</pre> result:</strong></p><pre>    ‚úÖ Test case passed</pre><p><strong><pre style='display: inline;'>get_data - 2</pre> result:</strong></p><pre>    ‚úÖ Test case passed</pre><p><strong><pre style='display: inline;'>get_data - 3</pre> result:</strong></p><pre>    ‚ùå Test case failed\n",
       "    Trying:\n",
       "        assert(np.count_nonzero(b_successful) == 355)\n",
       "    Expecting nothing\n",
       "    **********************************************************************\n",
       "    Line 1, in get_data 2\n",
       "    Failed example:\n",
       "        assert(np.count_nonzero(b_successful) == 355)\n",
       "    Exception raised:\n",
       "        Traceback (most recent call last):\n",
       "          File \"/workspaces/IntroPythonProgramming/.conda/lib/python3.12/doctest.py\", line 1361, in __run\n",
       "            exec(compile(example.source, filename, \"single\",\n",
       "          File \"<doctest get_data 2[0]>\", line 1, in <module>\n",
       "            assert(np.count_nonzero(b_successful) == 355)\n",
       "        AssertionError\n",
       "</pre>"
      ],
      "text/plain": [
       "get_data results:\n",
       "    get_data - 1 result:\n",
       "        ‚úÖ Test case passed\n",
       "\n",
       "    get_data - 2 result:\n",
       "        ‚úÖ Test case passed\n",
       "\n",
       "    get_data - 3 result:\n",
       "        ‚ùå Test case failed\n",
       "        Trying:\n",
       "            assert(np.count_nonzero(b_successful) == 355)\n",
       "        Expecting nothing\n",
       "        **********************************************************************\n",
       "        Line 1, in get_data 2\n",
       "        Failed example:\n",
       "            assert(np.count_nonzero(b_successful) == 355)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"/workspaces/IntroPythonProgramming/.conda/lib/python3.12/doctest.py\", line 1361, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest get_data 2[0]>\", line 1, in <module>\n",
       "                assert(np.count_nonzero(b_successful) == 355)\n",
       "            AssertionError"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"get_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Doing the slice\n",
    "\n",
    "Get the data for one of the channels. \n",
    "\n",
    "TODO: Copy over your function **get_channel_data** from lecture activity 2. Note: if your code did not handle doing 1, 2, or 3 dimensions, now you'll need it to. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This reads in the json data\n",
    "try:\n",
    "    with open(\"Data/proxy_data_description.json\", \"r\") as fp:\n",
    "        pick_data_description = json.load(fp)\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file was not found; check that the data directory is in the current one and the file is in that directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Copy get_channel_data over to here\n",
    "def get_channel_data(all_data, n_picks, start_index, n_time_steps, n_total_dims, n_dims):\n",
    "    \"\"\" Get the data for just one channel (eg, wrist torque)\n",
    "    @param all_data - the pick_channel_data numpy array\n",
    "    @param n_picks - number of picks (number of rows in all_data)\n",
    "    @param start_index - where to start getting data from \n",
    "    @param n_time_steps - number of time steps\n",
    "    @param n_total_dims - what the skip value is - the total number of channels\n",
    "    @param n_dims - total number of dimensions to use (1, 2, or 3)\n",
    "    @return Return array should be n_picks X (n_timesteps * n_dims)\"\"\"\n",
    "\n",
    "    # TODO Your slice code goes here. Note that I kept most of the variable names the same, so you should only have\n",
    "    #  to change the wrist torque specific ones\n",
    "\n",
    "    wrist_torque_data = np.zeros((int(n_picks), int(n_time_steps * n_dims)))\n",
    "\n",
    "    for item in range(0,3):\n",
    "        wrist_torque_data[:, item::start_index] = all_data[:, start_index + item::n_total_dims]\n",
    "    return (wrist_torque_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test 1 - the wrist torque data using hard-wired values\n",
    "wrist_torque_start_index = 3\n",
    "n_dims_wrist_torque = 3\n",
    "pick_channel_data = pick_data[:,:-1] \n",
    "\n",
    "wrist_torque_data = get_channel_data(pick_channel_data, \n",
    "                                     n_picks=n_picks, \n",
    "                                     start_index=wrist_torque_start_index,\n",
    "                                     n_time_steps=n_timesteps,\n",
    "                                     n_total_dims=n_total_dims,\n",
    "                                     n_dims=n_dims_wrist_torque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SELF TESTS\n",
    "# Feel free to copy over the asserts from lecture activity 2 to debug the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (660,40) into shape (660,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m motor_effort_f1_start_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[1;32m      3\u001b[0m n_dims_motor_effort_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m motor_effort_f1_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_channel_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpick_channel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mn_picks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_picks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmotor_effort_f1_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mn_time_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mn_total_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_total_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mn_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_dims_motor_effort_f1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 18\u001b[0m, in \u001b[0;36mget_channel_data\u001b[0;34m(all_data, n_picks, start_index, n_time_steps, n_total_dims, n_dims)\u001b[0m\n\u001b[1;32m     15\u001b[0m wrist_torque_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mint\u001b[39m(n_picks), \u001b[38;5;28mint\u001b[39m(n_time_steps \u001b[38;5;241m*\u001b[39m n_dims)))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mwrist_torque_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m all_data[:, start_index \u001b[38;5;241m+\u001b[39m item::n_total_dims]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (wrist_torque_data)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (660,40) into shape (660,3)"
     ]
    }
   ],
   "source": [
    "# Tests for Motor effort finger 1\n",
    "motor_effort_f1_start_index = 14\n",
    "n_dims_motor_effort_f1 = 1\n",
    "motor_effort_f1_data = get_channel_data(pick_channel_data, \n",
    "                                        n_picks=n_picks, \n",
    "                                        start_index=motor_effort_f1_start_index,\n",
    "                                        n_time_steps=n_timesteps,\n",
    "                                        n_total_dims=n_total_dims,\n",
    "                                        n_dims=n_dims_motor_effort_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'motor_effort_f1_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check size and first, last element\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[43mmotor_effort_f1_data\u001b[49m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (n_picks, n_timesteps \u001b[38;5;241m*\u001b[39m n_dims_motor_effort_f1))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(np\u001b[38;5;241m.\u001b[39misclose(motor_effort_f1_data[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0.0\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(np\u001b[38;5;241m.\u001b[39misclose(motor_effort_f1_data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m51.11\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'motor_effort_f1_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Check size and first, last element\n",
    "assert(motor_effort_f1_data.shape == (n_picks, n_timesteps * n_dims_motor_effort_f1))\n",
    "assert(np.isclose(motor_effort_f1_data[0, 0], 0.0))\n",
    "assert(np.isclose(motor_effort_f1_data[-1, -1], 51.11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong style='color: red;'><pre style='display: inline;'>check_slice</pre> results:</strong></p><p><strong><pre style='display: inline;'>check_slice - 1</pre> result:</strong></p><pre>    ‚ùå Test case failed\n",
       "    Trying:\n",
       "        assert(motor_effort_f1_data.shape == (n_picks, n_timesteps * n_dims_motor_effort_f1))\n",
       "    Expecting nothing\n",
       "    **********************************************************************\n",
       "    Line 1, in check_slice 0\n",
       "    Failed example:\n",
       "        assert(motor_effort_f1_data.shape == (n_picks, n_timesteps * n_dims_motor_effort_f1))\n",
       "    Exception raised:\n",
       "        Traceback (most recent call last):\n",
       "          File \"/workspaces/IntroPythonProgramming/.conda/lib/python3.12/doctest.py\", line 1361, in __run\n",
       "            exec(compile(example.source, filename, \"single\",\n",
       "          File \"<doctest check_slice 0[0]>\", line 1, in <module>\n",
       "            assert(motor_effort_f1_data.shape == (n_picks, n_timesteps * n_dims_motor_effort_f1))\n",
       "                   ^^^^^^^^^^^^^^^^^^^^\n",
       "        NameError: name 'motor_effort_f1_data' is not defined\n",
       "</pre><p><strong><pre style='display: inline;'>check_slice - 2</pre> result:</strong></p><pre>    ‚úÖ Test case passed</pre><p><strong><pre style='display: inline;'>check_slice - 3</pre> result:</strong></p><pre>    ‚ùå Test case failed\n",
       "    Trying:\n",
       "        assert(np.isclose(motor_effort_f1_data[0, 0], 0.0))\n",
       "    Expecting nothing\n",
       "    **********************************************************************\n",
       "    Line 1, in check_slice 2\n",
       "    Failed example:\n",
       "        assert(np.isclose(motor_effort_f1_data[0, 0], 0.0))\n",
       "    Exception raised:\n",
       "        Traceback (most recent call last):\n",
       "          File \"/workspaces/IntroPythonProgramming/.conda/lib/python3.12/doctest.py\", line 1361, in __run\n",
       "            exec(compile(example.source, filename, \"single\",\n",
       "          File \"<doctest check_slice 2[0]>\", line 1, in <module>\n",
       "            assert(np.isclose(motor_effort_f1_data[0, 0], 0.0))\n",
       "                              ^^^^^^^^^^^^^^^^^^^^\n",
       "        NameError: name 'motor_effort_f1_data' is not defined\n",
       "</pre><p><strong><pre style='display: inline;'>check_slice - 4</pre> result:</strong></p><pre>    ‚ùå Test case failed\n",
       "    Trying:\n",
       "        assert(np.isclose(motor_effort_f1_data[-1, -1], 51.11))\n",
       "    Expecting nothing\n",
       "    **********************************************************************\n",
       "    Line 1, in check_slice 3\n",
       "    Failed example:\n",
       "        assert(np.isclose(motor_effort_f1_data[-1, -1], 51.11))\n",
       "    Exception raised:\n",
       "        Traceback (most recent call last):\n",
       "          File \"/workspaces/IntroPythonProgramming/.conda/lib/python3.12/doctest.py\", line 1361, in __run\n",
       "            exec(compile(example.source, filename, \"single\",\n",
       "          File \"<doctest check_slice 3[0]>\", line 1, in <module>\n",
       "            assert(np.isclose(motor_effort_f1_data[-1, -1], 51.11))\n",
       "                              ^^^^^^^^^^^^^^^^^^^^\n",
       "        NameError: name 'motor_effort_f1_data' is not defined\n",
       "</pre>"
      ],
      "text/plain": [
       "check_slice results:\n",
       "    check_slice - 1 result:\n",
       "        ‚ùå Test case failed\n",
       "        Trying:\n",
       "            assert(motor_effort_f1_data.shape == (n_picks, n_timesteps * n_dims_motor_effort_f1))\n",
       "        Expecting nothing\n",
       "        **********************************************************************\n",
       "        Line 1, in check_slice 0\n",
       "        Failed example:\n",
       "            assert(motor_effort_f1_data.shape == (n_picks, n_timesteps * n_dims_motor_effort_f1))\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"/workspaces/IntroPythonProgramming/.conda/lib/python3.12/doctest.py\", line 1361, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest check_slice 0[0]>\", line 1, in <module>\n",
       "                assert(motor_effort_f1_data.shape == (n_picks, n_timesteps * n_dims_motor_effort_f1))\n",
       "                       ^^^^^^^^^^^^^^^^^^^^\n",
       "            NameError: name 'motor_effort_f1_data' is not defined\n",
       "\n",
       "    check_slice - 2 result:\n",
       "        ‚úÖ Test case passed\n",
       "\n",
       "    check_slice - 3 result:\n",
       "        ‚ùå Test case failed\n",
       "        Trying:\n",
       "            assert(np.isclose(motor_effort_f1_data[0, 0], 0.0))\n",
       "        Expecting nothing\n",
       "        **********************************************************************\n",
       "        Line 1, in check_slice 2\n",
       "        Failed example:\n",
       "            assert(np.isclose(motor_effort_f1_data[0, 0], 0.0))\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"/workspaces/IntroPythonProgramming/.conda/lib/python3.12/doctest.py\", line 1361, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest check_slice 2[0]>\", line 1, in <module>\n",
       "                assert(np.isclose(motor_effort_f1_data[0, 0], 0.0))\n",
       "                                  ^^^^^^^^^^^^^^^^^^^^\n",
       "            NameError: name 'motor_effort_f1_data' is not defined\n",
       "\n",
       "    check_slice - 4 result:\n",
       "        ‚ùå Test case failed\n",
       "        Trying:\n",
       "            assert(np.isclose(motor_effort_f1_data[-1, -1], 51.11))\n",
       "        Expecting nothing\n",
       "        **********************************************************************\n",
       "        Line 1, in check_slice 3\n",
       "        Failed example:\n",
       "            assert(np.isclose(motor_effort_f1_data[-1, -1], 51.11))\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"/workspaces/IntroPythonProgramming/.conda/lib/python3.12/doctest.py\", line 1361, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest check_slice 3[0]>\", line 1, in <module>\n",
       "                assert(np.isclose(motor_effort_f1_data[-1, -1], 51.11))\n",
       "                                  ^^^^^^^^^^^^^^^^^^^^\n",
       "            NameError: name 'motor_effort_f1_data' is not defined"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"check_slice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Compute stats: Write a function to calculate the four stats\n",
    "\n",
    "This is a variation on what you did in lab 1; in this case, we're going to do it with two functions. The first calculates the stats and returns the dictionary (**calc_stats**) the second does the **for** loop to make one dictionary for each dimension in the data.\n",
    "\n",
    "- Step 1 [this problem] - do the **calc_stats** function\n",
    "- Step 2 [next problem] - do the loop to calculate the stats for each x,y,z channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_stats(data):\n",
    "    \"\"\"Calculate min, max, mean and standard deviation for the array and put in a dictionary\n",
    "    @param data a numpy array\n",
    "    @return a dictionary\"\"\"\n",
    "\n",
    "    # Use keys Min, Max, Mean, and SD\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the function with known data\n",
    "test_data = np.linspace(0, 1, 10)\n",
    "ret_dict = calc_stats(test_data)\n",
    "\n",
    "assert(np.isclose(ret_dict[\"Min\"], 0.0))\n",
    "assert(np.isclose(ret_dict[\"Max\"], 1.0))\n",
    "assert(np.isclose(ret_dict[\"Mean\"], 0.5))\n",
    "assert(np.isclose(ret_dict[\"SD\"], 0.319, atol=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"stats_channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Now do the second half - \n",
    "\n",
    "This function calculates the stats for an entire channel of the data, and stores the result in a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_stats_for_channel(data, n_dims):\n",
    "    \"\"\" Calculate the stats for a channel\n",
    "    @param data - an n_picks X n_timesteps * n_dims size rray\n",
    "    @param n_dims - 1, 2, or 3 (just x, or x,y, and z)\n",
    "    @return A list of dictionaries. The list is the lenght of n_dims\"\"\"\n",
    "\n",
    "    stats_list = []\n",
    "    # TODO Copy in your for loop from the statistics problem in Lab 1\n",
    "    # - You do NOT need to get the data out from pick data - it's done for you\n",
    "    # - You DO need to slice the data into the x,y,z channels\n",
    "    # - You need to loop n_dims times\n",
    "    # - Don't forget to return the array\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SCRATCH CELL\n",
    "# If you're having trouble, try setting n_dims to 1 and use test_data for the data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing with known data - make a fake data set with 5 picks, 4 time steps, and x, y, z data\n",
    "#  \n",
    "test_stats = np.zeros((5, 4 * 3))\n",
    "# Set the x data to be ones\n",
    "test_stats[:, 0::3] = np.ones((5, 4))\n",
    "# Set the y data to be twos\n",
    "test_stats[:, 1::3] = np.ones((5, 4)) * 2\n",
    "# Set the z data to be threes\n",
    "test_stats[:, 2::3] = np.ones((5, 4)) * 3\n",
    "\n",
    "# Now get the actual stats\n",
    "ret_stats_array = calc_stats_for_channel(test_stats, n_dims=3)\n",
    "\n",
    "# Check the mean result for x, y, and z - should be 1, 2, and 3 respectively\n",
    "assert(ret_stats_array[0][\"Mean\"] == 1.0)\n",
    "assert(ret_stats_array[1][\"Mean\"] == 2.0)\n",
    "assert(ret_stats_array[2][\"Mean\"] == 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this should work - you can check the result against the values in Data/HW1_check_results.json\n",
    "ret_stats_wrist_torque = calc_stats_for_channel(wrist_torque_data, n_dims_wrist_torque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As should this\n",
    "res_stats_motor_effort_f1 = calc_stats_for_channel(motor_effort_f1_data, n_dims_motor_effort_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"loop_data_calc_stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Boolean slicing to get successful versus unsuccessful statistics out\n",
    "\n",
    "Use the functions you just wrote - plus the boolean index you made at the beginning - to get out the min and max z values for successful versus unsuccessful picks. \n",
    "\n",
    "For this problem I have written code that is *incorrect*. You know the functions themselves are correct - you just tested them. The following bits of code have something wrong with either the way the function is called OR with the way the results are gotten back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use b_successful to pick out the rows that are successful. Send all column data for the selected rows.\n",
    "#   Wrist torque data has 3 dimensions (x,y,z)\n",
    "#   There's two errors here - one that actually will create incorrect results, one that just *happens* to work\n",
    "#   correctly, although it doesn't do what the first sentence says...\n",
    "ret_wrist_torque_successful = calc_stats_for_channel(wrist_torque_data[b_successful], n_dims=1)\n",
    "\n",
    "# The minimum should be in the third (last) element in the list, the \"min\" key\n",
    "z_min_successful = ret_wrist_torque_successful[\"Min\"]\n",
    "z_max_successful = ret_wrist_torque_successful[\"Max\"]\n",
    "\n",
    "# Use b_successful NOT true to pick out the picks that are successful.\n",
    "#  This generates a weird error - it's because not does not work over a numpy array. Try b_successful == False instead\n",
    "ret_wrist_torque_unsuccessful = calc_stats_for_channel(wrist_torque_data[not b_successful], n_dims=1)\n",
    "\n",
    "# The minimum should be in the third (last) element in the list, the \"min\" key\n",
    "z_min_unsuccessful = ret_wrist_torque_unsuccessful[3][\"Min\"]\n",
    "# Why copying and pasting and changing variable names can cause problems...\n",
    "z_max_unsuccessful = ret_wrist_torque_successful[\"Max\"]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Successful: Minimum {ret_wrist_torque_successful} and maximum {z_max_successful} value of wrist torque z channel\")\n",
    "print(f\"Unsuccessful: Minimum {z_min_unsuccessful} and maximum {z_max_unsuccessful} value of wrist torque z channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"boolean_slicing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Optional/Extra credit: print out all of the indices where the maximum value for the successful pick was reached\n",
    "\n",
    "See the tutorial on **np.where**\n",
    "\n",
    "TODO: Use **np.where** to pick out the row, col pair that has the maximum z value of the successful pick\n",
    "\n",
    "Partial credit for picking out any row, col that has the maximum z value in **wrist_torque_data**, full extra credit for only printing out the row, col indices of successful picks with that *z* value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use np.where to get out the indices. You can use == OR np.isclose() here; either works. In general, use .isclose for \n",
    "#  floating point comparisons.\n",
    "# Append the row number of any matches to this list\n",
    "all_rows_with_max = []\n",
    "\n",
    "\n",
    "# Look at JUST the z values in wrist_torque_data\n",
    "all_indices_from_where = ...\n",
    "# Pseudo code - see tutorial for exact format\n",
    "# for all row, column in all_indices_from_where\n",
    "#.   if this is row is successful \n",
    "#.      print(f\"Row: {r}, Time step: {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"optional_where\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Hours and collaborators\n",
    "Required for every assignment - fill out before you hand-in.\n",
    "\n",
    "Listing names and websites helps you to document who you worked with and what internet help you received in the case of any plagiarism issues. You should list names of anyone (in class or not) who has substantially helped you with an assignment - or anyone you have *helped*. You do not need to list TAs.\n",
    "\n",
    "Listing hours helps us track if the assignments are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# List of names (creates a set)\n",
    "worked_with_names = {\"not filled out\"}\n",
    "# List of URLS TCS3 (creates a set)\n",
    "websites = {\"not filled out\"}\n",
    "# Approximate number of hours, including lab/in-class time\n",
    "hours = -1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"hours_collaborators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To submit\n",
    "\n",
    "- Do a restart then run all to make sure everything runs ok\n",
    "- Save the file\n",
    "- Submit just this .ipynb file through gradescope, Lab 2, functions\n",
    "- You do NOT need to submit the data files - we will supply those\n",
    "- Where there are given variable/file names (eg, foo = ...) DON'T change those, or the autograder will fail\n",
    "\n",
    "If the Gradescope autograder fails, please check here first for common reasons for it to fail\n",
    "    https://docs.google.com/presentation/d/1tYa5oycUiG4YhXUq5vHvPOpWJ4k_xUPp2rUNIL7Q9RI/edit?usp=sharing\n",
    "\n",
    "Most likely failure for this assignment is not naming the data directory and files correctly; capitalization matters for the Gradescope grader. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "boolean_slicing": {
     "name": "boolean_slicing",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(np.isclose(z_min_successful, -0.293665094))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(z_max_successful, 0.340460618))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(z_min_unsuccessful, -0.62552044))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(z_max_unsuccessful, 0.326538637))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "check_slice": {
     "name": "check_slice",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(motor_effort_f1_data.shape == (n_picks, n_timesteps * n_dims_motor_effort_f1))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(wrist_torque_data.shape == (n_picks, n_timesteps * n_dims_wrist_torque))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(motor_effort_f1_data[0, 0], 0.0))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(motor_effort_f1_data[-1, -1], 51.11))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "get_data": {
     "name": "get_data",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(pick_channel_data.shape[0] == 660)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(pick_channel_data.shape[1] == n_timesteps * n_total_dims)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.count_nonzero(b_successful) == 355)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "hours_collaborators": {
     "name": "hours_collaborators",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(not \"not filled out\" in worked_with_names)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(not \"not filled out\" in websites)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(hours > 0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "loop_data_calc_stats": {
     "name": "loop_data_calc_stats",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(len(ret_stats_array) == 3)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(ret_stats_array[0][\"Mean\"] == 1.0)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(ret_stats_array[1][\"Mean\"] == 2.0)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(ret_stats_array[2][\"Mean\"] == 3.0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "mystery_question": {
     "name": "mystery_question",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(np.isclose(mystery_variable, 3.5, atol=0.1))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(my_fibonacci == [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144])\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> find_me_weird_variable = {\"Hi\":0, \"how\":1, \"are\":2, \"you\":3}\n>>> assert(find_me_weird_variable[\"Hi\"] == 0)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "optional_where": {
     "name": "optional_where",
     "points": 0,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(all_rows_with_max[0] == 82)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "stats_channel": {
     "name": "stats_channel",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert(np.isclose(ret_dict[\"Min\"], 0.0))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(ret_dict[\"Max\"], 1.0))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(ret_dict[\"Mean\"], 0.5))\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert(np.isclose(ret_dict[\"SD\"], 0.319, atol=0.01))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
